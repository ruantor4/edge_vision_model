"""
yolo_evaluator.py

Evaluator especÃ­fico para modelos YOLO,
responsÃ¡vel por adaptar as prediÃ§Ãµes do YOLO para o formato
COCO e delegar a avaliaÃ§Ã£o ao base_coco_evaluator.

Responsabilidades:
- Executar inferÃªncia com YOLO
- Converter prediÃ§Ãµes YOLO (cx, cy, w, h) para COCO (x, y, w, h)
- Chamar o evaluator base (COCO)
- Retornar mÃ©tricas padronizadas (mAP / AR)

Este mÃ³dulo:
- NÃƒO treina modelos
- NÃƒO decide early stopping
- NÃƒO compara resultados
- Atua apenas como adaptador de saÃ­da
"""

from typing import Dict, List

import torch
from torch.utils.data import DataLoader

from pycocotools.coco import COCO

from evaluators.base_coco_evaluator import evaluate_coco


# ============================================================
# FUNÃ‡ÃƒO PRINCIPAL DE AVALIAÃ‡ÃƒO
# ============================================================

def evaluate_yolo(
    *,
    model,
    dataloader: DataLoader,
    coco_gt: COCO,
    device: torch.device,
    model_name: str,
    split_name: str,
    score_threshold: float = 0.05,
    save_metrics: bool = True,
) -> Dict[str, float]:
    """
    Avalia um modelo YOLO utilizando o protocolo COCO.

    ParÃ¢metros:
    - model: modelo YOLO jÃ¡ carregado
    - dataloader: DataLoader do split avaliado (valid ou test)
    - coco_gt: objeto COCO carregado com annotations.json
    - device: dispositivo de execuÃ§Ã£o (cpu ou cuda)
    - model_name: nome do modelo (ex: yolo)
    - split_name: nome do split avaliado (valid ou test)
    - score_threshold: score mÃ­nimo para considerar prediÃ§Ã£o
    - save_metrics: se True, salva mÃ©tricas em artifacts

    Retorna:
    - DicionÃ¡rio com mÃ©tricas COCO padronizadas
    """

    model.eval()

    predictions: List[Dict] = []

    with torch.no_grad():
        for images, targets in dataloader:
            # ====================================================
            # ðŸ”§ CONVERSÃƒO OBRIGATÃ“RIA
            # torch.Tensor (CHW, float) -> np.ndarray (HWC, uint8)
            # ====================================================
            np_images = []

            for img in images:
                img_np = img.permute(1, 2, 0).cpu().numpy()  # CHW -> HWC
                img_np = (img_np * 255).astype("uint8")      # float -> uint8
                np_images.append(img_np)

            # ====================================================
            # INFERÃŠNCIA YOLO (API CORRETA)
            # ====================================================
            results = model.predict(
                source=np_images,
                imgsz=640,
                conf=score_threshold,
                device=device,
                verbose=False,
            )

            # Itera por imagem do batch
            for i, (result, target) in enumerate(zip(results, targets)):
                if len(target) == 0:
                    continue

                image_id = int(target[0]["image_id"])

                # YOLO (Ultralytics) costuma expor .boxes
                boxes = result.boxes

                if boxes is None:
                    continue

                for box in boxes:
                    score = float(box.conf.item())
                    if score < score_threshold:
                        continue

                    cls_id = int(box.cls.item()) + 1
                    x_center, y_center, w, h = box.xywh[0].tolist()

                    # Converte YOLO (cx, cy, w, h) â†’ COCO (x, y, w, h)
                    x_min = x_center - (w / 2)
                    y_min = y_center - (h / 2)

                    predictions.append(
                        {
                            "image_id": image_id,
                            "category_id": cls_id,
                            "x": float(x_min),
                            "y": float(y_min),
                            "w": float(w),
                            "h": float(h),
                            "score": score,
                        }
                    )

    if len(predictions) == 0:
        raise ValueError(
            "Nenhuma prediÃ§Ã£o vÃ¡lida gerada pelo YOLO "
            "para avaliaÃ§Ã£o COCO"
        )

    metrics = evaluate_coco(
        coco_gt=coco_gt,
        predictions=predictions,
        model_name=model_name,
        split_name=split_name,
        save_metrics=save_metrics,
    )

    return metrics